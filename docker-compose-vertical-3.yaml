services:
########################## Hadoop ##########################
  namenode:
    image: hadoop-cluster
    hostname: namenode
    user: root
    command: ["./entrypoint.sh","namenode"]
    volumes:
      - ./data-stored/namenode-data:/hadoop/logs
      - ./data-stored/volume:/pipeline/source
    ports:
      - 54310:54310
      - 9870:9870
    networks:
      - hadoop-net
    deploy:
      resources:
        limits:
          memory: 2G
    

  datanode-1:
    image: hadoop-cluster
    hostname: datanode-1
    user: root
    command: ["./entrypoint.sh","datanode"]
    volumes:
      - ./data-stored/datanode-data-1:/datanode/datablocks
    deploy:
      resources:
        limits:
          memory: 512M
    
    ports:
      - 9864:9864
    depends_on:
      - namenode
    networks:
      - hadoop-net
    

  datanode-2:
    image: hadoop-cluster
    user: root
    hostname: datanode-2
    command: ["./entrypoint.sh","datanode"]
    volumes:
      - ./data-stored/datanode-data-2:/datanode/datablocks
    ports:
      - 9863:9864
    depends_on:
      - namenode
    networks:
      - hadoop-net
    deploy:
      resources:
        limits:
          memory: 512M
   
  
  datanode-3:
    image: hadoop-cluster
    user: root
    hostname: datanode-3
    command: ["./entrypoint.sh","datanode"]
    volumes:
      - ./data-stored/datanode-data-3:/datanode/datablocks 
    ports:
      - 9862:9864
    depends_on:
      - namenode
    networks:
      - hadoop-net
    deploy:
      resources:
        limits:
          memory: 512M    
  
  resourcemanager:
    image: hadoop-cluster
    hostname: resourcemanager
    user: root
    command: ["./entrypoint.sh","resourcemanager"]
    ports:
      - 8088:8088
    networks:
      - hadoop-net
    deploy:
      resources:
        limits:
          memory: 1G
 
  nodemanager:
    image: hadoop-cluster
    hostname: nodemanager
    user: root
    command: ["./entrypoint.sh","nodemanager"]
    networks:
      - hadoop-net
    deploy:
      resources:
        limits:
          memory: 512G

########################## Spark ##########################
  spark:
    image: spark-cluster
    hostname: spark
    command: ["spark-class", "org.apache.spark.deploy.master.Master"]
    volumes:
      - ./app:/app
      - ./data-stored/history-logs:/spark/history-logs
    ports:
      - 7077:7077
      - 8080:8080
      - 4040:4040
    networks:
      - hadoop-net
    deploy:
      resources:
        limits:
          memory: 2G

  spark-worker:
    image: spark-cluster
    hostname: spark-worker
    command: ["spark-class", "org.apache.spark.deploy.worker.Worker","spark://spark:7077"]
    depends_on:
      - spark
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=3
      - SPARK_WORKER_MEMORY=3G
      - SPARK_DRIVER_MEMORY=3G
      - SPARK_EXECUTOR_MEMORY=3G
      - SPARK_WORKLOAD=worker
      - SPARK_HISTORY_OPTS=-Dspark.history.fs.logDirectory=file:/spark-logs
    ports:
      - '8081'
    networks:
      - hadoop-net
    deploy:
      resources:
        limits:
          memory: 1G
  spark-history-server:
    image: spark-cluster
    hostname: spark-history-server
    volumes:  
      - ./data-stored/history-logs:/spark/history-logs
    deploy:
      resources:
        limits:
          memory: 1G

    depends_on:
      - spark
    command: ["spark-class", "org.apache.spark.deploy.history.HistoryServer"]
    ports:
      - 18080:18080
    networks:
      - hadoop-net
    environment:
      - SPARK_HISTORY_OPTS=-Dspark.history.fs.logDirectory=file:/spark/history-logs -Dspark.history.ui.port=18080
 

########################## nifi ##########################
  nifi-node:
    image: apache/nifi
    hostname: nifi-ingestion
    ports:
      - 9443:9443
      - 4883:4883
    networks:
      - hadoop-net
    volumes:
      - ./hadoop/config:/hadoop/conf
    environment:
    - SINGLE_USER_CREDENTIALS_USERNAME=admin 
    - SINGLE_USER_CREDENTIALS_PASSWORD=$Abd.Proj1-batch-Queries
    - NIFI_WEB_HTTP_PORT=4883
    - NIFI_WEB_HTTPS_PORT=9443

########################## Cassandra ##########################

  cassandra-1:
    image: "cassandra:latest" 
    container_name: "cassandra-1"
    ports:
      - 7000:7000
      - 9042:9042
    networks:
      - hadoop-net
    environment:
      - CASSANDRA_USER=admin
      - CASSANDRA_PASSWORD=admin
      - CASSANDRA_START_RPC=true       # default
      - CASSANDRA_RPC_ADDRESS=0.0.0.0  # default
      - CASSANDRA_LISTEN_ADDRESS=cassandra-1  # default, use IP addr of container # = CASSANDRA_BROADCAST_ADDRESS
      - CASSANDRA_CLUSTER_NAME=cassandra-cluster
      - CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch
      - CASSANDRA_DC=docker-compose-local
    volumes:
      - ./data-stored/cassandra-persistence/cassandra-node-1:/var/lib/cassandra:rw
    restart:
      on-failure
    healthcheck:
      test: ["CMD-SHELL", "nodetool status"]
      interval: 2m
      start_period: 2m
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 4G

  cassandra-2:
    image: "cassandra:latest" 
    container_name: "cassandra-2"
    ports:
      - 9043:9042
    networks:
      - hadoop-net
    environment:
      - CASSANDRA_START_RPC=true       # default
      - CASSANDRA_RPC_ADDRESS=0.0.0.0  # default
      - CASSANDRA_LISTEN_ADDRESS=cassandra-2  # default, use IP addr of container # = CASSANDRA_BROADCAST_ADDRESS
      - CASSANDRA_CLUSTER_NAME=cassandra-cluster
      - CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch
      - CASSANDRA_DC=docker-compose-local
      - CASSANDRA_SEEDS=cassandra-1
    depends_on:
      cassandra-1:
        condition: service_healthy
    volumes:
      - ./data-stored/cassandra-persistence/cassandra-node-2:/var/lib/cassandra:rw
    restart:
      on-failure
    healthcheck:
      test: ["CMD-SHELL", "nodetool status"]
      interval: 2m
      start_period: 2m
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 4G

  cassandra-3:
    image: "cassandra:latest" 
    container_name: "cassandra-3"
    ports:
      - 9044:9042
    networks:
      - hadoop-net
    environment:
      - CASSANDRA_START_RPC=true       # default
      - CASSANDRA_RPC_ADDRESS=0.0.0.0  # default
      - CASSANDRA_LISTEN_ADDRESS=cassandra-3 # default, use IP addr of container # = CASSANDRA_BROADCAST_ADDRESS
      - CASSANDRA_CLUSTER_NAME=cassandra-cluster
      - CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch
      - CASSANDRA_DC=docker-compose-local
      - CASSANDRA_SEEDS=cassandra-1
    depends_on:
      cassandra-2:
        condition: service_healthy
    volumes:
      - ./data-stored/cassandra-persistence/cassandra-node-3:/var/lib/cassandra:rw
    restart:
      on-failure
    healthcheck:
      test: ["CMD-SHELL", "nodetool status"]
      interval: 1m
      start_period: 10s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 4G

networks:
  hadoop-net:
    external: true

